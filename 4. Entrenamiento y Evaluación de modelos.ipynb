{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e11fadc2",
   "metadata": {},
   "source": [
    "# Pasos:\n",
    "\n",
    "1. Particionar datos de Entrenamiento (80%) y Pruebas(20%) \n",
    "2. Entrenar Modelos a evaluar:\n",
    "    - XGBoost\n",
    "    - SVM\n",
    "    - Ensamble Modelo (RandomForest o Arbol de decisión)\n",
    "    - Naive Bayes - Gausiano\n",
    "3. Evaluar los hiperparametros (RandomSearchCV)\n",
    "4. Entrenar modelos con mejores hiperparámetros encontrados en 3.\n",
    "5. Evaluar modelos con data de pruebas (Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75965c7c",
   "metadata": {},
   "source": [
    "## 1) Lectura de datos - Distribución de conjunto de datos\n",
    "\n",
    "La distribución del conjunto de datos será de la siguiente manera: \n",
    "\n",
    "- Entrenamiento (80%)\n",
    "- Pruebas (20%)\n",
    "\n",
    "--- Ya fue aplicado en 2. Pre-procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1815862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, neighbors\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4653298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura del dataset\n",
    "\n",
    "\n",
    "#Lectura del dataset\n",
    "\n",
    "X_TRAIN_DF = pd.read_csv('data/03_entrada_modelo/X_MI_entrenamiento_caracterizado.csv', header=0, sep=\",\")\n",
    "X_TEST_DF  = pd.read_csv('data/03_entrada_modelo/X_MI_pruebas_caracterizado.csv', header=0, sep=\",\")\n",
    "Y_TRAIN_DF = pd.read_csv('data/03_entrada_modelo/y_MI_entrenamiento_caracterizado.csv', header=0, sep=\",\")\n",
    "Y_TEST_DF  = pd.read_csv('data/03_entrada_modelo/y_MI_pruebas_caracterizado.csv', header=0, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05e111dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_objetivo_lst = [\n",
    "    'FIBR_PREDS','PREDS_TAH','JELUD_TAH','FIBR_JELUD',\n",
    "    'A_V_BLOK','OTEK_LANC','RAZRIV','DRESSLER','ZSN',\n",
    "    'REC_IM','P_IM_STEN'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a75200b",
   "metadata": {},
   "source": [
    "## 2) Entrenar Modelos a evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8dcc824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resultados con la métrica de exactitud (Accuracy) de cada uno de los modelos.\n",
    "resultados_modelo_acc_df = pd.DataFrame(columns=[\"Modelo\"]+var_objetivo_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18456e2",
   "metadata": {},
   "source": [
    "### 2.1) Random Forest\n",
    "\n",
    "Entrenamos el modelo de Random Forest de tipo clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4e4c576",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.959731543624161\n",
      "Exactitud del modelo inicial en validación: 0.9029411764705882\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': 52,\n",
      " 'max_features': 10,\n",
      " 'min_samples_leaf': 1,\n",
      " 'n_estimators': 1600}\n",
      "\n",
      "****************************************************************************************************\n",
      "FIBR_PREDS\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 1.0\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9635067114093959\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.8970588235294118\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.9946605644546148\n",
      "Exactitud del modelo inicial en validación: 0.9852941176470589\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': 136,\n",
      " 'max_features': 12,\n",
      " 'min_samples_leaf': 4,\n",
      " 'n_estimators': 1200}\n",
      "\n",
      "****************************************************************************************************\n",
      "PREDS_TAH\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 0.9984744469870328\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.994279176201373\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.9852941176470589\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.9910956252419667\n",
      "Exactitud del modelo inicial en validación: 0.9794117647058823\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': None,\n",
      " 'max_features': 11,\n",
      " 'min_samples_leaf': 1,\n",
      " 'n_estimators': 800}\n",
      "\n",
      "****************************************************************************************************\n",
      "JELUD_TAH\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 1.0\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9914827719705769\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.9794117647058823\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.9870486656200942\n",
      "Exactitud del modelo inicial en validación: 0.9470588235294117\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': 220,\n",
      " 'max_features': 'auto',\n",
      " 'min_samples_leaf': 2,\n",
      " 'n_estimators': 1400}\n",
      "\n",
      "****************************************************************************************************\n",
      "FIBR_JELUD\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 0.9968602825745683\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9850863422291993\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.9441176470588235\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.9910226385636222\n",
      "Exactitud del modelo inicial en validación: 0.9705882352941176\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': 52,\n",
      " 'max_features': 10,\n",
      " 'min_samples_leaf': 1,\n",
      " 'n_estimators': 1600}\n",
      "\n",
      "****************************************************************************************************\n",
      "A_V_BLOK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 1.0\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9910226385636222\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.9705882352941176\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.9579166666666666\n",
      "Exactitud del modelo inicial en validación: 0.9117647058823529\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': 52,\n",
      " 'max_features': 10,\n",
      " 'min_samples_leaf': 1,\n",
      " 'n_estimators': 1600}\n",
      "\n",
      "****************************************************************************************************\n",
      "OTEK_LANC\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 1.0\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9591666666666666\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.9058823529411765\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.9894695787831513\n",
      "Exactitud del modelo inicial en validación: 0.9705882352941176\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': 199,\n",
      " 'max_features': 'auto',\n",
      " 'min_samples_leaf': 2,\n",
      " 'n_estimators': 1000}\n",
      "\n",
      "****************************************************************************************************\n",
      "RAZRIV\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 0.998829953198128\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9894695787831513\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.9705882352941176\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.9838328075709779\n",
      "Exactitud del modelo inicial en validación: 0.95\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': 52,\n",
      " 'max_features': 10,\n",
      " 'min_samples_leaf': 1,\n",
      " 'n_estimators': 1600}\n",
      "\n",
      "****************************************************************************************************\n",
      "DRESSLER\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 1.0\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9830441640378549\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.95\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.8855333658061374\n",
      "Exactitud del modelo inicial en validación: 0.7647058823529411\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': 52,\n",
      " 'max_features': 10,\n",
      " 'min_samples_leaf': 1,\n",
      " 'n_estimators': 1600}\n",
      "\n",
      "****************************************************************************************************\n",
      "ZSN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 1.0\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.8923526546517292\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.7823529411764706\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.9568315171835708\n",
      "Exactitud del modelo inicial en validación: 0.9235294117647059\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': 52,\n",
      " 'max_features': 10,\n",
      " 'min_samples_leaf': 1,\n",
      " 'n_estimators': 1600}\n",
      "\n",
      "****************************************************************************************************\n",
      "REC_IM\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 1.0\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9589270746018441\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.9205882352941176\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.956953642384106\n",
      "Exactitud del modelo inicial en validación: 0.9117647058823529\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': None,\n",
      " 'max_features': 11,\n",
      " 'min_samples_leaf': 1,\n",
      " 'n_estimators': 800}\n",
      "\n",
      "****************************************************************************************************\n",
      "P_IM_STEN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 1.0\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9586092715231788\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.9088235294117647\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for var_objetivo in var_objetivo_lst:\n",
    "    \n",
    "    X_train = X_TRAIN_DF.loc[X_TRAIN_DF[\"var_objetivo\"]==var_objetivo].dropna(axis='columns')\n",
    "    y_train = Y_TRAIN_DF.loc[Y_TRAIN_DF[\"var_objetivo\"]==var_objetivo, var_objetivo]\n",
    "    X_test = X_TEST_DF.loc[X_TEST_DF[\"var_objetivo\"]==var_objetivo].dropna(axis='columns')\n",
    "    y_test = Y_TEST_DF.loc[Y_TEST_DF[\"var_objetivo\"]==var_objetivo, var_objetivo]\n",
    "    X_train.drop([\"var_objetivo\"], axis=1, inplace=True)\n",
    "    X_test.drop([\"var_objetivo\"], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    rf = RandomForestClassifier(oob_score = True)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    print('Exactitud del modelo inicial en entrenamiento:', rf.score(X_train, y_train))\n",
    "    print('Exactitud del modelo inicial en entrenamiento (Out of Bag):', rf.oob_score_)\n",
    "    print('Exactitud del modelo inicial en validación:', rf.score(X_test, y_test))\n",
    "\n",
    "    #### Búsqueda aleatoria de mejores hiperparámetros\n",
    "    # Definición de Grilla\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    max_features = ['auto', 10, 11, 12]  # 'auto' equivale a 'sqrt'; None equivale a todas\n",
    "    max_depth = [int(x) for x in np.linspace(10, 220, num = 11)] + [None]\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "    print('Los valores a probar en la búsqueda aleatoria son:')\n",
    "    pprint(random_grid)\n",
    "\n",
    "    print()\n",
    "    print('Si se probara todas las combinaciones se requeriría entrenar', \n",
    "          len(random_grid['n_estimators']) *\n",
    "          len(random_grid['max_features']) *\n",
    "          len(random_grid['max_depth']) *\n",
    "          len(random_grid['min_samples_leaf']),\n",
    "          'modelos'\n",
    "          )\n",
    "\n",
    "    rf = RandomForestClassifier(oob_score=True)\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, \n",
    "                                   param_distributions = random_grid, \n",
    "                                   cv = 3,          # Validación cruzada 3-fold\n",
    "                                   verbose=2, \n",
    "                                   random_state=0, \n",
    "                                   n_jobs = -1      # Paralelizar en todos los cores disponibles\n",
    "                                   )\n",
    "    rf_random.fit(X_train, y_train)\n",
    "\n",
    "    ## Tomamos el mejor estimador encontrado en la búsqueda aleatoria por grilla.\n",
    "    rf_random_best = rf_random.best_estimator_\n",
    "\n",
    "    print('Los hiperparámetros del mejor modelo son:')\n",
    "    pprint(rf_random.best_params_)\n",
    "    print()\n",
    "\n",
    "    resultados_modelo_acc_df=resultados_modelo_acc_df.append(\n",
    "        {\n",
    "            'Modelo' : 'RandomForest' , \n",
    "            'Accuracy Train' : rf_random_best.score(X_train, y_train),\n",
    "            'Accuracy Test' : rf_random_best.score(X_test, y_test)\n",
    "        },\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "    print('*'*100)\n",
    "    print(f'{var_objetivo}')\n",
    "    print('-'*100)\n",
    "    print('Exactitud luego de búsqueda aleatoria en entrenamiento:', rf_random_best.score(X_train, y_train))\n",
    "    print('Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag):', rf_random_best.oob_score_)\n",
    "    print('Exactitud luego de búsqueda aleatoria en validación:', rf_random_best.score(X_test, y_test))\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d169fc0a",
   "metadata": {},
   "source": [
    "### 2.2) Ensamble - XGBoost\n",
    "\n",
    "Entrenamos el modelo de XGBoost de tipo clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aff1f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "## XGBoostRegressor\n",
    "modelXGBR = XGBRegressor()\n",
    "\n",
    "\n",
    "## Creamos la Grilla\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 5)]\n",
    "learning_rate = [0.01,0.1]\n",
    "max_depth = [i for i in range(2,8,2)]\n",
    "start = time.time()\n",
    "\n",
    "random_grid = {'max_depth': max_depth,\n",
    "               'n_estimators': n_estimators,\n",
    "               'colsample_bytree': [0.2, 0.6, 0.8],\n",
    "               'min_child_weight': [3, 5, 7],\n",
    "               'gamma': [0.3, 0.5, 0.7],\n",
    "               'subsample': [0.4, 0.6, 0.8, 1],\n",
    "               'learning_rate': learning_rate}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7267be37",
   "metadata": {},
   "source": [
    "#### Búsqueda aleatoria de mejores hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90514f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                          colsample_bylevel=None,\n",
       "                                          colsample_bynode=None,\n",
       "                                          colsample_bytree=None,\n",
       "                                          enable_categorical=False, gamma=None,\n",
       "                                          gpu_id=None, importance_type=None,\n",
       "                                          interaction_constraints=None,\n",
       "                                          learning_rate=None,\n",
       "                                          max_delta_step=None, max_depth=None,\n",
       "                                          min_child_weight=None, missing=nan,\n",
       "                                          monotone_constraints=...\n",
       "                                          reg_alpha=None, reg_lambda=None,\n",
       "                                          scale_pos_weight=None, subsample=None,\n",
       "                                          tree_method=None,\n",
       "                                          validate_parameters=None,\n",
       "                                          verbosity=None),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;colsample_bytree&#x27;: [0.2, 0.6, 0.8],\n",
       "                                        &#x27;gamma&#x27;: [0.3, 0.5, 0.7],\n",
       "                                        &#x27;learning_rate&#x27;: [0.01, 0.1],\n",
       "                                        &#x27;max_depth&#x27;: [2, 4, 6],\n",
       "                                        &#x27;min_child_weight&#x27;: [3, 5, 7],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 200, 300, 400,\n",
       "                                                         500],\n",
       "                                        &#x27;subsample&#x27;: [0.4, 0.6, 0.8, 1]},\n",
       "                   random_state=0, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                          colsample_bylevel=None,\n",
       "                                          colsample_bynode=None,\n",
       "                                          colsample_bytree=None,\n",
       "                                          enable_categorical=False, gamma=None,\n",
       "                                          gpu_id=None, importance_type=None,\n",
       "                                          interaction_constraints=None,\n",
       "                                          learning_rate=None,\n",
       "                                          max_delta_step=None, max_depth=None,\n",
       "                                          min_child_weight=None, missing=nan,\n",
       "                                          monotone_constraints=...\n",
       "                                          reg_alpha=None, reg_lambda=None,\n",
       "                                          scale_pos_weight=None, subsample=None,\n",
       "                                          tree_method=None,\n",
       "                                          validate_parameters=None,\n",
       "                                          verbosity=None),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;colsample_bytree&#x27;: [0.2, 0.6, 0.8],\n",
       "                                        &#x27;gamma&#x27;: [0.3, 0.5, 0.7],\n",
       "                                        &#x27;learning_rate&#x27;: [0.01, 0.1],\n",
       "                                        &#x27;max_depth&#x27;: [2, 4, 6],\n",
       "                                        &#x27;min_child_weight&#x27;: [3, 5, 7],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 200, 300, 400,\n",
       "                                                         500],\n",
       "                                        &#x27;subsample&#x27;: [0.4, 0.6, 0.8, 1]},\n",
       "                   random_state=0, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n",
       "             colsample_bynode=None, colsample_bytree=None,\n",
       "             enable_categorical=False, gamma=None, gpu_id=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, reg_alpha=None, reg_lambda=None,\n",
       "             scale_pos_weight=None, subsample=None, tree_method=None,\n",
       "             validate_parameters=None, verbosity=None)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n",
       "             colsample_bynode=None, colsample_bytree=None,\n",
       "             enable_categorical=False, gamma=None, gpu_id=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, reg_alpha=None, reg_lambda=None,\n",
       "             scale_pos_weight=None, subsample=None, tree_method=None,\n",
       "             validate_parameters=None, verbosity=None)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                          colsample_bylevel=None,\n",
       "                                          colsample_bynode=None,\n",
       "                                          colsample_bytree=None,\n",
       "                                          enable_categorical=False, gamma=None,\n",
       "                                          gpu_id=None, importance_type=None,\n",
       "                                          interaction_constraints=None,\n",
       "                                          learning_rate=None,\n",
       "                                          max_delta_step=None, max_depth=None,\n",
       "                                          min_child_weight=None, missing=nan,\n",
       "                                          monotone_constraints=...\n",
       "                                          reg_alpha=None, reg_lambda=None,\n",
       "                                          scale_pos_weight=None, subsample=None,\n",
       "                                          tree_method=None,\n",
       "                                          validate_parameters=None,\n",
       "                                          verbosity=None),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.2, 0.6, 0.8],\n",
       "                                        'gamma': [0.3, 0.5, 0.7],\n",
       "                                        'learning_rate': [0.01, 0.1],\n",
       "                                        'max_depth': [2, 4, 6],\n",
       "                                        'min_child_weight': [3, 5, 7],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500],\n",
       "                                        'subsample': [0.4, 0.6, 0.8, 1]},\n",
       "                   random_state=0, verbose=2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_random = RandomizedSearchCV(estimator = modelXGBR, \n",
    "                               param_distributions = random_grid, \n",
    "                               cv = 3,          # Validación cruzada 3-fold\n",
    "                               verbose=2, \n",
    "                               random_state=0, \n",
    "                               n_jobs = -1      # Paralelizar en todos los cores disponibles\n",
    "                               )\n",
    "xgb_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1522eba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los hiperparámetros del mejor modelo son:\n",
      "{'colsample_bytree': 0.2,\n",
      " 'gamma': 0.7,\n",
      " 'learning_rate': 0.01,\n",
      " 'max_depth': 4,\n",
      " 'min_child_weight': 7,\n",
      " 'n_estimators': 400,\n",
      " 'subsample': 1}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 30.552%\n",
      "Exactitud luego de búsqueda en grilla en validación: 5.303%\n",
      "****************************************************************************************************\n",
      "15.436229467391968 segundos\n"
     ]
    }
   ],
   "source": [
    "## Tomamos el mejor estimador encontrado en la búsqueda aleatoria por grilla.\n",
    "xgb_best_model = xgb_random.best_estimator_\n",
    "\n",
    "print('Los hiperparámetros del mejor modelo son:')\n",
    "pprint(xgb_random.best_params_)\n",
    "print()\n",
    "\n",
    "\n",
    "score_XGBR_train = xgb_best_model.score(X_train, y_train)\n",
    "score_XGBR_val = xgb_best_model.score(X_test, y_test)\n",
    "print(\"*\"*100)\n",
    "\n",
    "resultados_modelo_acc_df=resultados_modelo_acc_df.append(\n",
    "    {\n",
    "        'Modelo' : 'XGBoost' , \n",
    "        'Accuracy Train' : score_XGBR_train,\n",
    "        'Accuracy Test' : score_XGBR_val\n",
    "    },\n",
    "    ignore_index=True\n",
    ")\n",
    "print(f\"Exactitud luego de búsqueda en grilla en entrenamiento: {score_XGBR_train*100:.3f}%\")\n",
    "print(f\"Exactitud luego de búsqueda en grilla en validación: {score_XGBR_val*100:.3f}%\")\n",
    "print(\"*\"*100)\n",
    "end = time.time()\n",
    "print(f\"{end-start} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6760d65",
   "metadata": {},
   "source": [
    "### 2.3) SVM\n",
    "\n",
    "Entrenamos el modelo de SVM de tipo clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2f15711",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud con SVM base - Entrenamiento: 0.4588235294117647\n",
      "Exactitud con SVM base - Pruebas: 0.47352941176470587\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "modelSVM = svm.SVC()\n",
    "\n",
    "# Entrenamiento del modelo base - SVM\n",
    "modelSVM.fit(X_train, y_train)\n",
    "score_SVM_train = modelSVM.score(X_train, y_train)\n",
    "score_SVM_test = modelSVM.score(X_test, y_test)\n",
    "\n",
    "\n",
    "print(f\"Exactitud con SVM base - Entrenamiento: {score_SVM_train}\")\n",
    "print(f\"Exactitud con SVM base - Pruebas: {score_SVM_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b00f9e",
   "metadata": {},
   "source": [
    "#### Búsqueda aleatoria de mejores hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "322c6074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 10 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=12.\n",
      "  warnings.warn(\n",
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los hiperparámetros del mejor modelo son:\n",
      "{'C': 0.01, 'gamma': 1.0, 'kernel': 'rbf'}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.444\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.474\n",
      "97.53587985038757\n"
     ]
    }
   ],
   "source": [
    "# Definición de grilla\n",
    "random_grid = {'kernel': ['rbf'],\n",
    "               'C': np.logspace(-4,4,9), # [0.0001, 0.001, ..., 10000]\n",
    "               'gamma': np.logspace(-4,4,9)  # [0.0001, 0.001, ..., 10000]\n",
    "              }\n",
    "\n",
    "modelSVM_random = RandomizedSearchCV(estimator = modelSVM, \n",
    "                               param_distributions = random_grid, \n",
    "                               cv = 12,          # Validación cruzada 3-fold\n",
    "                               verbose=2, \n",
    "                               random_state=0, \n",
    "                               n_jobs = -1      # Paralelizar en todos los cores disponibles\n",
    "                               )\n",
    "modelSVM_random.fit(X_train, y_train)\n",
    "\n",
    "## Tomamos el mejor estimador encontrado en la búsqueda aleatoria por grilla.\n",
    "modelSVM_best_model = modelSVM_random.best_estimator_\n",
    "\n",
    "print('Los hiperparámetros del mejor modelo son:')\n",
    "pprint(modelSVM_random.best_params_)\n",
    "print()\n",
    "\n",
    "\n",
    "score_SVM_train = modelSVM_best_model.score(X_train, y_train)\n",
    "score_SVM_test = modelSVM_best_model.score(X_test, y_test)\n",
    "print(\"*\"*100)\n",
    "\n",
    "resultados_modelo_acc_df=resultados_modelo_acc_df.append(\n",
    "    {\n",
    "        'Modelo' : 'SVM' ,\n",
    "        'Accuracy Train' : score_SVM_train,\n",
    "        'Accuracy Test' : score_SVM_test\n",
    "    },\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(f\"Exactitud luego de búsqueda en grilla en entrenamiento: {score_SVM_train:.3f}\")\n",
    "print(f\"Exactitud luego de búsqueda en grilla en validación: {score_SVM_test:.3f}\")\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1170c46",
   "metadata": {},
   "source": [
    "### 2.4. Naive Bayes\n",
    "\n",
    "Entrenamos el modelo de Naive Bayes (Gausiano) de tipo clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7ba7ee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad8b479d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'var_smoothing': array([1.00000000e+00, 8.11130831e-01, 6.57933225e-01, 5.33669923e-01,\n",
      "       4.32876128e-01, 3.51119173e-01, 2.84803587e-01, 2.31012970e-01,\n",
      "       1.87381742e-01, 1.51991108e-01, 1.23284674e-01, 1.00000000e-01,\n",
      "       8.11130831e-02, 6.57933225e-02, 5.33669923e-02, 4.32876128e-02,\n",
      "       3.51119173e-02, 2.84803587e-02, 2.31012970e-02, 1.87381742e-02,\n",
      "       1.51991108e-02, 1.23284674e-02, 1.00000000e-02, 8.11130831e-03,\n",
      "       6.57933225e-03, 5.33669923e-03, 4.32876128e-03, 3.51119173e-03,\n",
      "       2.84803587e-03, 2.31012970e-03, 1.87381742e-03, 1.51991108e-03,\n",
      "       1.23284674e-03, 1.00000000e-03, 8.11130831e-04, 6.57933225e-04,\n",
      "       5.33669923e-04, 4.32876128e-04, 3.51119173e-04, 2.84803587e-04,\n",
      "       2.31012970e-04, 1.87381742e-04, 1.51991108e-04, 1.23284674e-04,\n",
      "       1.00000000e-04, 8.11130831e-05, 6.57933225e-05, 5.33669923e-05,\n",
      "       4.32876128e-05, 3.51119173e-05, 2.84803587e-05, 2.31012970e-05,\n",
      "       1.87381742e-05, 1.51991108e-05, 1.23284674e-05, 1.00000000e-05,\n",
      "       8.11130831e-06, 6.57933225e-06, 5.33669923e-06, 4.32876128e-06,\n",
      "       3.51119173e-06, 2.84803587e-06, 2.31012970e-06, 1.87381742e-06,\n",
      "       1.51991108e-06, 1.23284674e-06, 1.00000000e-06, 8.11130831e-07,\n",
      "       6.57933225e-07, 5.33669923e-07, 4.32876128e-07, 3.51119173e-07,\n",
      "       2.84803587e-07, 2.31012970e-07, 1.87381742e-07, 1.51991108e-07,\n",
      "       1.23284674e-07, 1.00000000e-07, 8.11130831e-08, 6.57933225e-08,\n",
      "       5.33669923e-08, 4.32876128e-08, 3.51119173e-08, 2.84803587e-08,\n",
      "       2.31012970e-08, 1.87381742e-08, 1.51991108e-08, 1.23284674e-08,\n",
      "       1.00000000e-08, 8.11130831e-09, 6.57933225e-09, 5.33669923e-09,\n",
      "       4.32876128e-09, 3.51119173e-09, 2.84803587e-09, 2.31012970e-09,\n",
      "       1.87381742e-09, 1.51991108e-09, 1.23284674e-09, 1.00000000e-09])}\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n",
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los hiperparámetros del mejor modelo son:\n",
      "{'var_smoothing': 0.657933224657568}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.388\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.282\n"
     ]
    }
   ],
   "source": [
    "modelNB = GaussianNB()\n",
    "#modelNB.fit(X_train, y_train)\n",
    "\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "\n",
    "print('Los valores a probar en la búsqueda aleatoria son:')\n",
    "pprint(params_NB)\n",
    "\n",
    "modelNB_random = RandomizedSearchCV(estimator = modelNB, \n",
    "                               param_distributions=params_NB, \n",
    "                               cv = 3,          # Validación cruzada 3-fold\n",
    "                               verbose=2, \n",
    "                               random_state=0, \n",
    "                               n_jobs = -1      # Paralelizar en todos los cores disponibles\n",
    "                               )\n",
    "modelNB_random.fit(X_train, y_train)\n",
    "\n",
    "## Tomamos el mejor estimador encontrado en la búsqueda aleatoria por grilla.\n",
    "modelNB_best_model = modelNB_random.best_estimator_\n",
    "\n",
    "print('Los hiperparámetros del mejor modelo son:')\n",
    "pprint(modelNB_random.best_params_)\n",
    "print()\n",
    "\n",
    "\n",
    "score_NB_train = modelNB_best_model.score(X_train, y_train)\n",
    "score_NB_test = modelNB_best_model.score(X_test, y_test)\n",
    "print(\"*\"*100)\n",
    "\n",
    "resultados_modelo_acc_df=resultados_modelo_acc_df.append(\n",
    "    {\n",
    "        'Modelo' : 'Naive Bayes - Gaussian' ,\n",
    "        'Accuracy Train' : score_NB_train,\n",
    "        'Accuracy Test' : score_NB_test\n",
    "    },\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(f\"Exactitud luego de búsqueda en grilla en entrenamiento: {score_NB_train:.3f}\")\n",
    "print(f\"Exactitud luego de búsqueda en grilla en validación: {score_NB_test:.3f}\")\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f146cde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Accuracy Train</th>\n",
       "      <th>Accuracy Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.491176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.305517</td>\n",
       "      <td>0.053027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.444118</td>\n",
       "      <td>0.473529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes - Gaussian</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.282353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Modelo  Accuracy Train  Accuracy Test\n",
       "0            RandomForest        0.937500       0.491176\n",
       "1                 XGBoost        0.305517       0.053027\n",
       "2                     SVM        0.444118       0.473529\n",
       "3  Naive Bayes - Gaussian        0.388235       0.282353"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados_modelo_acc_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
