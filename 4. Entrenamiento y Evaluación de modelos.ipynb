{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e11fadc2",
   "metadata": {},
   "source": [
    "# Pasos:\n",
    "\n",
    "1. Particionar datos de Entrenamiento (80%) y Pruebas(20%) \n",
    "2. Entrenar Modelos a evaluar:\n",
    "    - XGBoost\n",
    "    - SVM\n",
    "    - Ensamble Modelo (RandomForest o Arbol de decisión)\n",
    "    - Naive Bayes - Gausiano\n",
    "3. Evaluar los hiperparametros (RandomSearchCV)\n",
    "4. Entrenar modelos con mejores hiperparámetros encontrados en 3.\n",
    "5. Evaluar modelos con data de pruebas (Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75965c7c",
   "metadata": {},
   "source": [
    "## 1) Lectura de datos - Distribución de conjunto de datos\n",
    "\n",
    "La distribución del conjunto de datos será de la siguiente manera: \n",
    "\n",
    "- Entrenamiento (80%)\n",
    "- Pruebas (20%)\n",
    "\n",
    "--- Ya fue aplicado en 2. Pre-procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1815862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, neighbors\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4653298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura del dataset\n",
    "\n",
    "\n",
    "#Lectura del dataset\n",
    "\n",
    "X_TRAIN_DF = pd.read_csv('data/03_entrada_modelo/X_MI_entrenamiento_caracterizado.csv', header=0, sep=\",\")\n",
    "X_TEST_DF  = pd.read_csv('data/03_entrada_modelo/X_MI_pruebas_caracterizado.csv', header=0, sep=\",\")\n",
    "Y_TRAIN_DF = pd.read_csv('data/03_entrada_modelo/y_MI_entrenamiento_caracterizado.csv', header=0, sep=\",\")\n",
    "Y_TEST_DF  = pd.read_csv('data/03_entrada_modelo/y_MI_pruebas_caracterizado.csv', header=0, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e111dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_objetivo_lst = [\n",
    "    'FIBR_PREDS','PREDS_TAH','JELUD_TAH','FIBR_JELUD',\n",
    "    'A_V_BLOK','OTEK_LANC','RAZRIV','DRESSLER','ZSN',\n",
    "    'REC_IM','P_IM_STEN'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a75200b",
   "metadata": {},
   "source": [
    "## 2) Entrenar Modelos a evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dcc824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resultados con la métrica de exactitud (Accuracy) de cada uno de los modelos.\n",
    "resultados_modelo_metricas_df = pd.DataFrame(columns=['Variable Objetivo','Modelo','Accuracy Train','Accuracy Test', 'Especificidad', \"F1_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18456e2",
   "metadata": {},
   "source": [
    "### 2.1) Random Forest\n",
    "\n",
    "Entrenamos el modelo de Random Forest de tipo clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e4c576",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.9576342281879194\n",
      "Exactitud del modelo inicial en validación: 0.9029411764705882\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': 52,\n",
      " 'max_features': 10,\n",
      " 'min_samples_leaf': 1,\n",
      " 'n_estimators': 1600}\n",
      "\n",
      "****************************************************************************************************\n",
      "FIBR_PREDS\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 1.0\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9639261744966443\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.8970588235294118\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9837133550488599\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.8676880347925269\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.9946605644546148\n",
      "Exactitud del modelo inicial en validación: 0.9852941176470589\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': 136,\n",
      " 'max_features': 10,\n",
      " 'min_samples_leaf': 2,\n",
      " 'n_estimators': 800}\n",
      "\n",
      "****************************************************************************************************\n",
      "PREDS_TAH\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 0.9984744469870328\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9946605644546148\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.9852941176470589\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 1.0\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.9779956427015251\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.991869918699187\n",
      "Exactitud del modelo inicial en validación: 0.9794117647058823\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': 136,\n",
      " 'max_features': 10,\n",
      " 'min_samples_leaf': 2,\n",
      " 'n_estimators': 800}\n",
      "\n",
      "****************************************************************************************************\n",
      "JELUD_TAH\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 0.9984514130855594\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9914827719705769\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.9794117647058823\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 1.0\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.9692247181190454\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.9858712715855573\n",
      "Exactitud del modelo inicial en validación: 0.9470588235294117\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': 52,\n",
      " 'max_features': 10,\n",
      " 'min_samples_leaf': 1,\n",
      " 'n_estimators': 1600}\n",
      "\n",
      "****************************************************************************************************\n",
      "FIBR_JELUD\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 1.0\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9843014128728415\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.9441176470588235\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9968944099378882\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.919836255228264\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.9894613583138173\n",
      "Exactitud del modelo inicial en validación: 0.9705882352941176\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': None,\n",
      " 'max_features': 11,\n",
      " 'min_samples_leaf': 1,\n",
      " 'n_estimators': 800}\n",
      "\n",
      "****************************************************************************************************\n",
      "A_V_BLOK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 1.0\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9894613583138173\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.9705882352941176\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 1.0\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.9585534812727485\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.9591666666666666\n",
      "Exactitud del modelo inicial en validación: 0.9147058823529411\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': None,\n",
      " 'max_features': 11,\n",
      " 'min_samples_leaf': 1,\n",
      " 'n_estimators': 800}\n",
      "\n",
      "****************************************************************************************************\n",
      "OTEK_LANC\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 1.0\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.95875\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.9058823529411765\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9838709677419355\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.8802503785577194\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.9890795631825273\n",
      "Exactitud del modelo inicial en validación: 0.9705882352941176\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': 220,\n",
      " 'max_features': 'auto',\n",
      " 'min_samples_leaf': 2,\n",
      " 'n_estimators': 1400}\n",
      "\n",
      "****************************************************************************************************\n",
      "RAZRIV\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 0.998829953198128\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9894695787831513\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.9676470588235294\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9939577039274925\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.9575222017057944\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.9822555205047319\n",
      "Exactitud del modelo inicial en validación: 0.9529411764705882\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': None,\n",
      " 'max_features': 11,\n",
      " 'min_samples_leaf': 1,\n",
      " 'n_estimators': 800}\n",
      "\n",
      "****************************************************************************************************\n",
      "DRESSLER\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 1.0\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9814668769716088\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.95\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9969135802469136\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.9285067873303167\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.8884559181685339\n",
      "Exactitud del modelo inicial en validación: 0.7647058823529411\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': None,\n",
      " 'max_features': 11,\n",
      " 'min_samples_leaf': 1,\n",
      " 'n_estimators': 800}\n",
      "\n",
      "****************************************************************************************************\n",
      "ZSN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 1.0\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.8938139308329274\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.7852941176470588\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9681274900398407\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.7456694406203175\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 1.0\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.9576697401508801\n",
      "Exactitud del modelo inicial en validación: 0.9235294117647059\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': 52,\n",
      " 'max_features': 10,\n",
      " 'min_samples_leaf': 1,\n",
      " 'n_estimators': 1600}\n",
      "\n",
      "****************************************************************************************************\n",
      "REC_IM\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 1.0\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9589270746018441\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.9205882352941176\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.990506329113924\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.890982794342852\n",
      "****************************************************************************************************\n",
      "Exactitud del modelo inicial en entrenamiento: 0.9995860927152318\n",
      "Exactitud del modelo inicial en entrenamiento (Out of Bag): 0.9594370860927153\n",
      "Exactitud del modelo inicial en validación: 0.9058823529411765\n",
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'max_depth': [10, 31, 52, 73, 94, 115, 136, 157, 178, 199, 220, None],\n",
      " 'max_features': ['auto', 10, 11, 12],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "\n",
      "Si se probara todas las combinaciones se requeriría entrenar 1440 modelos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'max_depth': 52,\n",
      " 'max_features': 10,\n",
      " 'min_samples_leaf': 1,\n",
      " 'n_estimators': 1600}\n",
      "\n",
      "****************************************************************************************************\n",
      "P_IM_STEN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento: 1.0\n",
      "Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag): 0.9619205298013245\n",
      "Exactitud luego de búsqueda aleatoria en validación: 0.9088235294117647\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9871794871794872\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.8786705127463377\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for var_objetivo in var_objetivo_lst:\n",
    "    \n",
    "    X_train = X_TRAIN_DF.loc[X_TRAIN_DF[\"var_objetivo\"]==var_objetivo].dropna(axis='columns')\n",
    "    y_train = Y_TRAIN_DF.loc[Y_TRAIN_DF[\"var_objetivo\"]==var_objetivo, var_objetivo]\n",
    "    X_test = X_TEST_DF.loc[X_TEST_DF[\"var_objetivo\"]==var_objetivo].dropna(axis='columns')\n",
    "    y_test = Y_TEST_DF.loc[Y_TEST_DF[\"var_objetivo\"]==var_objetivo, var_objetivo]\n",
    "    X_train.drop([\"var_objetivo\"], axis=1, inplace=True)\n",
    "    X_test.drop([\"var_objetivo\"], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    rf = RandomForestClassifier(oob_score = True)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    print('Exactitud del modelo inicial en entrenamiento:', rf.score(X_train, y_train))\n",
    "    print('Exactitud del modelo inicial en entrenamiento (Out of Bag):', rf.oob_score_)\n",
    "    print('Exactitud del modelo inicial en validación:', rf.score(X_test, y_test))\n",
    "\n",
    "    #### Búsqueda aleatoria de mejores hiperparámetros\n",
    "    # Definición de Grilla\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    max_features = ['auto', 10, 11, 12]  # 'auto' equivale a 'sqrt'; None equivale a todas\n",
    "    max_depth = [int(x) for x in np.linspace(10, 220, num = 11)] + [None]\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "    print('Los valores a probar en la búsqueda aleatoria son:')\n",
    "    pprint(random_grid)\n",
    "\n",
    "    print()\n",
    "    print('Si se probara todas las combinaciones se requeriría entrenar', \n",
    "          len(random_grid['n_estimators']) *\n",
    "          len(random_grid['max_features']) *\n",
    "          len(random_grid['max_depth']) *\n",
    "          len(random_grid['min_samples_leaf']),\n",
    "          'modelos'\n",
    "          )\n",
    "\n",
    "    rf = RandomForestClassifier(oob_score=True)\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, \n",
    "                                   param_distributions = random_grid, \n",
    "                                   cv = 3,          # Validación cruzada 3-fold\n",
    "                                   verbose=2, \n",
    "                                   random_state=0, \n",
    "                                   n_jobs = -1      # Paralelizar en todos los cores disponibles\n",
    "                                   )\n",
    "    rf_random.fit(X_train, y_train)\n",
    "\n",
    "    ## Tomamos el mejor estimador encontrado en la búsqueda aleatoria por grilla.\n",
    "    rf_random_best = rf_random.best_estimator_\n",
    "\n",
    "    print('Los hiperparámetros del mejor modelo son:')\n",
    "    pprint(rf_random.best_params_)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    y_pred = rf_random_best.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    f1_score_metric = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    resultados_modelo_metricas_df=resultados_modelo_metricas_df.append(\n",
    "        {\n",
    "            'Variable Objetivo' : var_objetivo,\n",
    "            'Modelo' : 'RandomForest' , \n",
    "            'Accuracy Train' : rf_random_best.score(X_train, y_train),\n",
    "            'Accuracy Test' : rf_random_best.score(X_test, y_test),\n",
    "            'Especificidad' : specificity,\n",
    "            'F1_score': f1_score_metric\n",
    "        },\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "    print('*'*100)\n",
    "    print(f'{var_objetivo}')\n",
    "    print('-'*100)\n",
    "    print('Exactitud luego de búsqueda aleatoria en entrenamiento:', rf_random_best.score(X_train, y_train))\n",
    "    print('Exactitud luego de búsqueda aleatoria en entrenamiento (Out of Bag):', rf_random_best.oob_score_)\n",
    "    print('Exactitud luego de búsqueda aleatoria en validación:', rf_random_best.score(X_test, y_test))\n",
    "    print('.'*50)\n",
    "    print('Especificidad luego de búsqueda aleatoria en validación:', specificity)\n",
    "    print('F1_score luego de búsqueda aleatoria en validación:', f1_score_metric)\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d169fc0a",
   "metadata": {},
   "source": [
    "### 2.2) Ensamble - XGBoost\n",
    "\n",
    "Entrenamos el modelo de XGBoost de tipo clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aff1f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "## XGBoostRegressor\n",
    "modelXGBC = XGBClassifier()\n",
    "\n",
    "\n",
    "## Creamos la Grilla\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 5)]\n",
    "learning_rate = [0.01,0.1]\n",
    "max_depth = [i for i in range(2,8,2)]\n",
    "start = time.time()\n",
    "\n",
    "random_grid = {'max_depth': max_depth,\n",
    "               'n_estimators': n_estimators,\n",
    "               'colsample_bytree': [0.2, 0.6, 0.8],\n",
    "               'min_child_weight': [3, 5, 7],\n",
    "               'gamma': [0.3, 0.5, 0.7],\n",
    "               'subsample': [0.4, 0.6, 0.8, 1],\n",
    "               'learning_rate': learning_rate}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7267be37",
   "metadata": {},
   "source": [
    "#### Búsqueda aleatoria de mejores hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90514f94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:19:05] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'colsample_bytree': 0.6,\n",
      " 'gamma': 0.5,\n",
      " 'learning_rate': 0.1,\n",
      " 'max_depth': 6,\n",
      " 'min_child_weight': 5,\n",
      " 'n_estimators': 500,\n",
      " 'subsample': 0.8}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 99.958%\n",
      "Exactitud luego de búsqueda en grilla en validación: 87.941%\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9674267100977199\n",
      "F1_score luego de búsqueda aleatoria en validación: <function f1_score at 0x000001DE83D5C8B0>\n",
      "****************************************************************************************************\n",
      "21.294273853302002 segundos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:19:19] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'colsample_bytree': 0.2,\n",
      " 'gamma': 0.7,\n",
      " 'learning_rate': 0.1,\n",
      " 'max_depth': 6,\n",
      " 'min_child_weight': 3,\n",
      " 'n_estimators': 400,\n",
      " 'subsample': 0.6}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 99.733%\n",
      "Exactitud luego de búsqueda en grilla en validación: 98.529%\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 1.0\n",
      "F1_score luego de búsqueda aleatoria en validación: <function f1_score at 0x000001DE83D5C8B0>\n",
      "****************************************************************************************************\n",
      "33.799187660217285 segundos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:19:32] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'colsample_bytree': 0.6,\n",
      " 'gamma': 0.5,\n",
      " 'learning_rate': 0.1,\n",
      " 'max_depth': 6,\n",
      " 'min_child_weight': 5,\n",
      " 'n_estimators': 500,\n",
      " 'subsample': 0.8}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 99.845%\n",
      "Exactitud luego de búsqueda en grilla en validación: 96.765%\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.987987987987988\n",
      "F1_score luego de búsqueda aleatoria en validación: <function f1_score at 0x000001DE83D5C8B0>\n",
      "****************************************************************************************************\n",
      "47.02131414413452 segundos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:19:44] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'colsample_bytree': 0.2,\n",
      " 'gamma': 0.7,\n",
      " 'learning_rate': 0.01,\n",
      " 'max_depth': 4,\n",
      " 'min_child_weight': 7,\n",
      " 'n_estimators': 400,\n",
      " 'subsample': 1}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 98.155%\n",
      "Exactitud luego de búsqueda en grilla en validación: 94.118%\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9906832298136646\n",
      "F1_score luego de búsqueda aleatoria en validación: <function f1_score at 0x000001DE83D5C8B0>\n",
      "****************************************************************************************************\n",
      "59.14498734474182 segundos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:19:58] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'colsample_bytree': 0.6,\n",
      " 'gamma': 0.5,\n",
      " 'learning_rate': 0.1,\n",
      " 'max_depth': 6,\n",
      " 'min_child_weight': 5,\n",
      " 'n_estimators': 500,\n",
      " 'subsample': 0.8}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 99.883%\n",
      "Exactitud luego de búsqueda en grilla en validación: 97.059%\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 1.0\n",
      "F1_score luego de búsqueda aleatoria en validación: <function f1_score at 0x000001DE83D5C8B0>\n",
      "****************************************************************************************************\n",
      "73.71425104141235 segundos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:20:19] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'colsample_bytree': 0.2,\n",
      " 'gamma': 0.7,\n",
      " 'learning_rate': 0.1,\n",
      " 'max_depth': 6,\n",
      " 'min_child_weight': 3,\n",
      " 'n_estimators': 400,\n",
      " 'subsample': 0.6}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 99.167%\n",
      "Exactitud luego de búsqueda en grilla en validación: 88.824%\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9741935483870968\n",
      "F1_score luego de búsqueda aleatoria en validación: <function f1_score at 0x000001DE83D5C8B0>\n",
      "****************************************************************************************************\n",
      "94.69938039779663 segundos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:20:31] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'colsample_bytree': 0.6,\n",
      " 'gamma': 0.5,\n",
      " 'learning_rate': 0.1,\n",
      " 'max_depth': 4,\n",
      " 'min_child_weight': 7,\n",
      " 'n_estimators': 300,\n",
      " 'subsample': 1}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 99.649%\n",
      "Exactitud luego de búsqueda en grilla en validación: 96.471%\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9909365558912386\n",
      "F1_score luego de búsqueda aleatoria en validación: <function f1_score at 0x000001DE83D5C8B0>\n",
      "****************************************************************************************************\n",
      "105.98776888847351 segundos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:20:44] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'colsample_bytree': 0.2,\n",
      " 'gamma': 0.7,\n",
      " 'learning_rate': 0.1,\n",
      " 'max_depth': 6,\n",
      " 'min_child_weight': 3,\n",
      " 'n_estimators': 400,\n",
      " 'subsample': 0.6}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 99.172%\n",
      "Exactitud luego de búsqueda en grilla en validación: 95.294%\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 1.0\n",
      "F1_score luego de búsqueda aleatoria en validación: <function f1_score at 0x000001DE83D5C8B0>\n",
      "****************************************************************************************************\n",
      "119.16726684570312 segundos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:20:59] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'colsample_bytree': 0.2,\n",
      " 'gamma': 0.7,\n",
      " 'learning_rate': 0.01,\n",
      " 'max_depth': 4,\n",
      " 'min_child_weight': 7,\n",
      " 'n_estimators': 400,\n",
      " 'subsample': 1}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 88.358%\n",
      "Exactitud luego de búsqueda en grilla en validación: 78.235%\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9840637450199203\n",
      "F1_score luego de búsqueda aleatoria en validación: <function f1_score at 0x000001DE83D5C8B0>\n",
      "****************************************************************************************************\n",
      "133.39063501358032 segundos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:21:13] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'colsample_bytree': 0.2,\n",
      " 'gamma': 0.7,\n",
      " 'learning_rate': 0.1,\n",
      " 'max_depth': 6,\n",
      " 'min_child_weight': 3,\n",
      " 'n_estimators': 400,\n",
      " 'subsample': 0.6}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 99.246%\n",
      "Exactitud luego de búsqueda en grilla en validación: 91.176%\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9778481012658228\n",
      "F1_score luego de búsqueda aleatoria en validación: <function f1_score at 0x000001DE83D5C8B0>\n",
      "****************************************************************************************************\n",
      "148.42719316482544 segundos\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\coliverac\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:21:31] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'colsample_bytree': 0.6,\n",
      " 'gamma': 0.5,\n",
      " 'learning_rate': 0.1,\n",
      " 'max_depth': 4,\n",
      " 'min_child_weight': 7,\n",
      " 'n_estimators': 300,\n",
      " 'subsample': 1}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 98.717%\n",
      "Exactitud luego de búsqueda en grilla en validación: 90.588%\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9839743589743589\n",
      "F1_score luego de búsqueda aleatoria en validación: <function f1_score at 0x000001DE83D5C8B0>\n",
      "****************************************************************************************************\n",
      "165.49558663368225 segundos\n"
     ]
    }
   ],
   "source": [
    "for var_objetivo in var_objetivo_lst:    \n",
    "    \n",
    "    X_train = X_TRAIN_DF.loc[X_TRAIN_DF[\"var_objetivo\"]==var_objetivo].dropna(axis='columns')\n",
    "    y_train = Y_TRAIN_DF.loc[Y_TRAIN_DF[\"var_objetivo\"]==var_objetivo, var_objetivo]\n",
    "    X_test = X_TEST_DF.loc[X_TEST_DF[\"var_objetivo\"]==var_objetivo].dropna(axis='columns')\n",
    "    y_test = Y_TEST_DF.loc[Y_TEST_DF[\"var_objetivo\"]==var_objetivo, var_objetivo]\n",
    "    X_train.drop([\"var_objetivo\"], axis=1, inplace=True)\n",
    "    X_test.drop([\"var_objetivo\"], axis=1, inplace=True)\n",
    "    \n",
    "    xgb_random = RandomizedSearchCV(estimator = modelXGBC, \n",
    "                                   param_distributions = random_grid, \n",
    "                                   cv = 3,          # Validación cruzada 3-fold\n",
    "                                   verbose=2, \n",
    "                                   random_state=0, \n",
    "                                   n_jobs = -1      # Paralelizar en todos los cores disponibles\n",
    "                                   )\n",
    "    xgb_random.fit(X_train, y_train)\n",
    "    ## Tomamos el mejor estimador encontrado en la búsqueda aleatoria por grilla.\n",
    "    xgb_best_model = xgb_random.best_estimator_\n",
    "\n",
    "    print('Los hiperparámetros del mejor modelo son:')\n",
    "    pprint(xgb_random.best_params_)\n",
    "    print()\n",
    "\n",
    "\n",
    "    y_pred = xgb_best_model.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    f1_score_metric = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    score_XGBR_train = xgb_best_model.score(X_train, y_train)\n",
    "    score_XGBR_val = xgb_best_model.score(X_test, y_test)\n",
    "    \n",
    "    resultados_modelo_metricas_df=resultados_modelo_metricas_df.append(\n",
    "        {\n",
    "            'Variable Objetivo' : var_objetivo,\n",
    "            'Modelo' : 'XGBoost' , \n",
    "            'Accuracy Train' : score_XGBR_train,\n",
    "            'Accuracy Test' : score_XGBR_val,\n",
    "            'Especificidad' : specificity,\n",
    "            'F1_score': f1_score_metric\n",
    "        },\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    print(\"*\"*100)\n",
    "    print(f\"Exactitud luego de búsqueda en grilla en entrenamiento: {score_XGBR_train*100:.3f}%\")\n",
    "    print(f\"Exactitud luego de búsqueda en grilla en validación: {score_XGBR_val*100:.3f}%\")\n",
    "    print('.'*50)\n",
    "    print('Especificidad luego de búsqueda aleatoria en validación:', specificity)\n",
    "    print('F1_score luego de búsqueda aleatoria en validación:', f1_score_metric)\n",
    "    print('*'*100)\n",
    "    end = time.time()\n",
    "    print(f\"{end-start} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6760d65",
   "metadata": {},
   "source": [
    "### 2.3) SVM\n",
    "\n",
    "Entrenamos el modelo de SVM de tipo clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2f15711",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "modelSVM = svm.SVC()\n",
    "# Entrenamiento del modelo base - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b00f9e",
   "metadata": {},
   "source": [
    "#### Búsqueda aleatoria de mejores hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "322c6074",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 10 candidates, totalling 120 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'C': 10000.0, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 1.000\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.812\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.8827361563517915\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.8206971094565735\n",
      "****************************************************************************************************\n",
      "352.1364185810089\n",
      "Fitting 12 folds for each of 10 candidates, totalling 120 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'C': 10000.0, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 1.000\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.974\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9850746268656716\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.974712836622623\n",
      "****************************************************************************************************\n",
      "384.1547267436981\n",
      "Fitting 12 folds for each of 10 candidates, totalling 120 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'C': 10000.0, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 1.000\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.956\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.975975975975976\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.9573197700132684\n",
      "****************************************************************************************************\n",
      "413.582453250885\n",
      "Fitting 12 folds for each of 10 candidates, totalling 120 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'C': 10000.0, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 1.000\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.921\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9596273291925466\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.919515341020129\n",
      "****************************************************************************************************\n",
      "441.6706893444061\n",
      "Fitting 12 folds for each of 10 candidates, totalling 120 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'C': 10000.0, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 1.000\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.929\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9574468085106383\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.934625762796297\n",
      "****************************************************************************************************\n",
      "472.5375802516937\n",
      "Fitting 12 folds for each of 10 candidates, totalling 120 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'C': 10000.0, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.999\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.865\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.932258064516129\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.8603075845722904\n",
      "****************************************************************************************************\n",
      "502.75119805336\n",
      "Fitting 12 folds for each of 10 candidates, totalling 120 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'C': 10000.0, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 1.000\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.953\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9788519637462235\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.9500708717221827\n",
      "****************************************************************************************************\n",
      "531.2448999881744\n",
      "Fitting 12 folds for each of 10 candidates, totalling 120 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'C': 10000.0, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.999\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.882\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9259259259259259\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.8933823529411766\n",
      "****************************************************************************************************\n",
      "560.9169156551361\n",
      "Fitting 12 folds for each of 10 candidates, totalling 120 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'C': 10000.0, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.993\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.653\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.7888446215139442\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.6444481519577968\n",
      "****************************************************************************************************\n",
      "589.3169231414795\n",
      "Fitting 12 folds for each of 10 candidates, totalling 120 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'C': 10000.0, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.999\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.812\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.8639240506329114\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.8379501308720211\n",
      "****************************************************************************************************\n",
      "620.033196926117\n",
      "Fitting 12 folds for each of 10 candidates, totalling 120 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'C': 10000.0, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.997\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.835\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.9006410256410257\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.8424642139975314\n",
      "****************************************************************************************************\n",
      "652.1364543437958\n"
     ]
    }
   ],
   "source": [
    "for var_objetivo in var_objetivo_lst:\n",
    "    # Lectura\n",
    "    X_train = X_TRAIN_DF.loc[X_TRAIN_DF[\"var_objetivo\"]==var_objetivo].dropna(axis='columns')\n",
    "    y_train = Y_TRAIN_DF.loc[Y_TRAIN_DF[\"var_objetivo\"]==var_objetivo, var_objetivo]\n",
    "    X_test = X_TEST_DF.loc[X_TEST_DF[\"var_objetivo\"]==var_objetivo].dropna(axis='columns')\n",
    "    y_test = Y_TEST_DF.loc[Y_TEST_DF[\"var_objetivo\"]==var_objetivo, var_objetivo]\n",
    "    X_train.drop([\"var_objetivo\"], axis=1, inplace=True)\n",
    "    X_test.drop([\"var_objetivo\"], axis=1, inplace=True)\n",
    "    \n",
    "    # Definición de grilla\n",
    "    random_grid = {'kernel': ['rbf'],\n",
    "                   'C': np.logspace(-4,4,9), # [0.0001, 0.001, ..., 10000]\n",
    "                   'gamma': np.logspace(-4,4,9)  # [0.0001, 0.001, ..., 10000]\n",
    "                  }\n",
    "\n",
    "    modelSVM_random = RandomizedSearchCV(estimator = modelSVM, \n",
    "                                   param_distributions = random_grid, \n",
    "                                   cv = 12,          # Validación cruzada 3-fold\n",
    "                                   verbose=2, \n",
    "                                   random_state=0, \n",
    "                                   n_jobs = -1      # Paralelizar en todos los cores disponibles\n",
    "                                   )\n",
    "    modelSVM_random.fit(X_train, y_train)\n",
    "\n",
    "    ## Tomamos el mejor estimador encontrado en la búsqueda aleatoria por grilla.\n",
    "    modelSVM_best_model = modelSVM_random.best_estimator_\n",
    "\n",
    "    print('Los hiperparámetros del mejor modelo son:')\n",
    "    pprint(modelSVM_random.best_params_)\n",
    "    print()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    y_pred = modelSVM_best_model.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    f1_score_metric = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    score_SVM_train = modelSVM_best_model.score(X_train, y_train)\n",
    "    score_SVM_test = modelSVM_best_model.score(X_test, y_test)\n",
    "    \n",
    "    resultados_modelo_metricas_df=resultados_modelo_metricas_df.append(\n",
    "        {\n",
    "            'Variable Objetivo' : var_objetivo,\n",
    "            'Modelo' : 'SVM' , \n",
    "            'Accuracy Train' : score_SVM_train,\n",
    "            'Accuracy Test' : score_SVM_test,\n",
    "            'Especificidad' : specificity,\n",
    "            'F1_score': f1_score_metric\n",
    "        },\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(\"*\"*100)\n",
    "\n",
    "    print(f\"Exactitud luego de búsqueda en grilla en entrenamiento: {score_SVM_train:.3f}\")\n",
    "    print(f\"Exactitud luego de búsqueda en grilla en validación: {score_SVM_test:.3f}\")\n",
    "    print('.'*50)\n",
    "    print('Especificidad luego de búsqueda aleatoria en validación:', specificity)\n",
    "    print('F1_score luego de búsqueda aleatoria en validación:', f1_score_metric)\n",
    "    print('*'*100)\n",
    "    end = time.time()\n",
    "    print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1170c46",
   "metadata": {},
   "source": [
    "### 2.4. Naive Bayes\n",
    "\n",
    "Entrenamos el modelo de Naive Bayes (Gausiano) de tipo clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7ba7ee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad8b479d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los valores a probar en la búsqueda aleatoria son:\n",
      "{'var_smoothing': array([1.00000000e+00, 8.11130831e-01, 6.57933225e-01, 5.33669923e-01,\n",
      "       4.32876128e-01, 3.51119173e-01, 2.84803587e-01, 2.31012970e-01,\n",
      "       1.87381742e-01, 1.51991108e-01, 1.23284674e-01, 1.00000000e-01,\n",
      "       8.11130831e-02, 6.57933225e-02, 5.33669923e-02, 4.32876128e-02,\n",
      "       3.51119173e-02, 2.84803587e-02, 2.31012970e-02, 1.87381742e-02,\n",
      "       1.51991108e-02, 1.23284674e-02, 1.00000000e-02, 8.11130831e-03,\n",
      "       6.57933225e-03, 5.33669923e-03, 4.32876128e-03, 3.51119173e-03,\n",
      "       2.84803587e-03, 2.31012970e-03, 1.87381742e-03, 1.51991108e-03,\n",
      "       1.23284674e-03, 1.00000000e-03, 8.11130831e-04, 6.57933225e-04,\n",
      "       5.33669923e-04, 4.32876128e-04, 3.51119173e-04, 2.84803587e-04,\n",
      "       2.31012970e-04, 1.87381742e-04, 1.51991108e-04, 1.23284674e-04,\n",
      "       1.00000000e-04, 8.11130831e-05, 6.57933225e-05, 5.33669923e-05,\n",
      "       4.32876128e-05, 3.51119173e-05, 2.84803587e-05, 2.31012970e-05,\n",
      "       1.87381742e-05, 1.51991108e-05, 1.23284674e-05, 1.00000000e-05,\n",
      "       8.11130831e-06, 6.57933225e-06, 5.33669923e-06, 4.32876128e-06,\n",
      "       3.51119173e-06, 2.84803587e-06, 2.31012970e-06, 1.87381742e-06,\n",
      "       1.51991108e-06, 1.23284674e-06, 1.00000000e-06, 8.11130831e-07,\n",
      "       6.57933225e-07, 5.33669923e-07, 4.32876128e-07, 3.51119173e-07,\n",
      "       2.84803587e-07, 2.31012970e-07, 1.87381742e-07, 1.51991108e-07,\n",
      "       1.23284674e-07, 1.00000000e-07, 8.11130831e-08, 6.57933225e-08,\n",
      "       5.33669923e-08, 4.32876128e-08, 3.51119173e-08, 2.84803587e-08,\n",
      "       2.31012970e-08, 1.87381742e-08, 1.51991108e-08, 1.23284674e-08,\n",
      "       1.00000000e-08, 8.11130831e-09, 6.57933225e-09, 5.33669923e-09,\n",
      "       4.32876128e-09, 3.51119173e-09, 2.84803587e-09, 2.31012970e-09,\n",
      "       1.87381742e-09, 1.51991108e-09, 1.23284674e-09, 1.00000000e-09])}\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'var_smoothing': 0.03511191734215131}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.718\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.606\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.6156351791530945\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.6862673975840335\n",
      "****************************************************************************************************\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'var_smoothing': 0.03511191734215131}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.696\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.429\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.4298507462686567\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.5890205447198369\n",
      "****************************************************************************************************\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'var_smoothing': 0.03511191734215131}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.821\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.771\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.7837837837837838\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.8526029411764705\n",
      "****************************************************************************************************\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'var_smoothing': 0.03511191734215131}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.785\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.718\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.7236024844720497\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.7951540396603783\n",
      "****************************************************************************************************\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'var_smoothing': 0.03511191734215131}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.728\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.544\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.5379939209726444\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.6760076449840268\n",
      "****************************************************************************************************\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'var_smoothing': 0.03511191734215131}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.665\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.515\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.4967741935483871\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.61161006364123\n",
      "****************************************************************************************************\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'var_smoothing': 0.004328761281083057}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.821\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.750\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.7643504531722054\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.834702180649822\n",
      "****************************************************************************************************\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'var_smoothing': 0.03511191734215131}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.720\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.550\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.5493827160493827\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.6714507289835591\n",
      "****************************************************************************************************\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'var_smoothing': 0.004328761281083057}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.617\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.418\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.3665338645418327\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.44343028050673516\n",
      "****************************************************************************************************\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Los hiperparámetros del mejor modelo son:\n",
      "{'var_smoothing': 0.03511191734215131}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.658\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.421\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.3987341772151899\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.5320193758815532\n",
      "****************************************************************************************************\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los hiperparámetros del mejor modelo son:\n",
      "{'var_smoothing': 0.03511191734215131}\n",
      "\n",
      "****************************************************************************************************\n",
      "Exactitud luego de búsqueda en grilla en entrenamiento: 0.702\n",
      "Exactitud luego de búsqueda en grilla en validación: 0.485\n",
      "..................................................\n",
      "Especificidad luego de búsqueda aleatoria en validación: 0.46474358974358976\n",
      "F1_score luego de búsqueda aleatoria en validación: 0.5876174926084464\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "modelNB = GaussianNB()\n",
    "#modelNB.fit(X_train, y_train)\n",
    "\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "\n",
    "print('Los valores a probar en la búsqueda aleatoria son:')\n",
    "pprint(params_NB)\n",
    "\n",
    "\n",
    "for var_objetivo in var_objetivo_lst:\n",
    "    # Lectura\n",
    "    X_train = X_TRAIN_DF.loc[X_TRAIN_DF[\"var_objetivo\"]==var_objetivo].dropna(axis='columns')\n",
    "    y_train = Y_TRAIN_DF.loc[Y_TRAIN_DF[\"var_objetivo\"]==var_objetivo, var_objetivo]\n",
    "    X_test = X_TEST_DF.loc[X_TEST_DF[\"var_objetivo\"]==var_objetivo].dropna(axis='columns')\n",
    "    y_test = Y_TEST_DF.loc[Y_TEST_DF[\"var_objetivo\"]==var_objetivo, var_objetivo]\n",
    "    X_train.drop([\"var_objetivo\"], axis=1, inplace=True)\n",
    "    X_test.drop([\"var_objetivo\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    modelNB_random = RandomizedSearchCV(estimator = modelNB, \n",
    "                                   param_distributions=params_NB, \n",
    "                                   cv = 3,          # Validación cruzada 3-fold\n",
    "                                   verbose=2, \n",
    "                                   random_state=0, \n",
    "                                   n_jobs = -1      # Paralelizar en todos los cores disponibles\n",
    "                                   )\n",
    "    modelNB_random.fit(X_train, y_train)\n",
    "\n",
    "    ## Tomamos el mejor estimador encontrado en la búsqueda aleatoria por grilla.\n",
    "    modelNB_best_model = modelNB_random.best_estimator_\n",
    "\n",
    "    print('Los hiperparámetros del mejor modelo son:')\n",
    "    pprint(modelNB_random.best_params_)\n",
    "    print()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    y_pred = modelNB_best_model.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    f1_score_metric = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    score_NB_train = modelNB_best_model.score(X_train, y_train)\n",
    "    score_NB_test = modelNB_best_model.score(X_test, y_test)\n",
    "    \n",
    "    resultados_modelo_metricas_df=resultados_modelo_metricas_df.append(\n",
    "        {\n",
    "            'Variable Objetivo' : var_objetivo,\n",
    "            'Modelo' : 'Naive Bayes - Gaussian' , \n",
    "            'Accuracy Train' : score_NB_train,\n",
    "            'Accuracy Test' : score_NB_test,\n",
    "            'Especificidad' : specificity,\n",
    "            'F1_score': f1_score_metric\n",
    "        },\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "    print(\"*\"*100)\n",
    "\n",
    "\n",
    "    print(f\"Exactitud luego de búsqueda en grilla en entrenamiento: {score_NB_train:.3f}\")\n",
    "    print(f\"Exactitud luego de búsqueda en grilla en validación: {score_NB_test:.3f}\")\n",
    "    print('.'*50)\n",
    "    print('Especificidad luego de búsqueda aleatoria en validación:', specificity)\n",
    "    print('F1_score luego de búsqueda aleatoria en validación:', f1_score_metric)\n",
    "    print('*'*100)\n",
    "    end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f146cde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable Objetivo</th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Accuracy Train</th>\n",
       "      <th>Accuracy Test</th>\n",
       "      <th>Especificidad</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FIBR_PREDS</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.897059</td>\n",
       "      <td>0.983713</td>\n",
       "      <td>0.867688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PREDS_TAH</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.998474</td>\n",
       "      <td>0.985294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JELUD_TAH</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.998451</td>\n",
       "      <td>0.979412</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FIBR_JELUD</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.944118</td>\n",
       "      <td>0.996894</td>\n",
       "      <td>0.919836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A_V_BLOK</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OTEK_LANC</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>0.880250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RAZRIV</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.998830</td>\n",
       "      <td>0.967647</td>\n",
       "      <td>0.993958</td>\n",
       "      <td>0.957522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DRESSLER</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.996914</td>\n",
       "      <td>0.928507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ZSN</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.785294</td>\n",
       "      <td>0.968127</td>\n",
       "      <td>0.745669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>REC_IM</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920588</td>\n",
       "      <td>0.990506</td>\n",
       "      <td>0.890983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>P_IM_STEN</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.908824</td>\n",
       "      <td>0.987179</td>\n",
       "      <td>0.878671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FIBR_PREDS</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.999581</td>\n",
       "      <td>0.879412</td>\n",
       "      <td>0.967427</td>\n",
       "      <td>0.853268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PREDS_TAH</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.997330</td>\n",
       "      <td>0.985294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>JELUD_TAH</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.998451</td>\n",
       "      <td>0.967647</td>\n",
       "      <td>0.987988</td>\n",
       "      <td>0.963308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FIBR_JELUD</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.981554</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.990683</td>\n",
       "      <td>0.923086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A_V_BLOK</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.998829</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>OTEK_LANC</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.888235</td>\n",
       "      <td>0.974194</td>\n",
       "      <td>0.857797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RAZRIV</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.996490</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.990937</td>\n",
       "      <td>0.956041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DRESSLER</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.991719</td>\n",
       "      <td>0.952941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.929979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ZSN</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.883585</td>\n",
       "      <td>0.782353</td>\n",
       "      <td>0.984064</td>\n",
       "      <td>0.730870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>REC_IM</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.992456</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.977848</td>\n",
       "      <td>0.890795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>P_IM_STEN</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.987169</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.983974</td>\n",
       "      <td>0.877035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FIBR_PREDS</td>\n",
       "      <td>SVM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.882736</td>\n",
       "      <td>0.820697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PREDS_TAH</td>\n",
       "      <td>SVM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973529</td>\n",
       "      <td>0.985075</td>\n",
       "      <td>0.974713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>JELUD_TAH</td>\n",
       "      <td>SVM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955882</td>\n",
       "      <td>0.975976</td>\n",
       "      <td>0.957320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>FIBR_JELUD</td>\n",
       "      <td>SVM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920588</td>\n",
       "      <td>0.959627</td>\n",
       "      <td>0.919515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>A_V_BLOK</td>\n",
       "      <td>SVM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.929412</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.934626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>OTEK_LANC</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.999167</td>\n",
       "      <td>0.864706</td>\n",
       "      <td>0.932258</td>\n",
       "      <td>0.860308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>RAZRIV</td>\n",
       "      <td>SVM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952941</td>\n",
       "      <td>0.978852</td>\n",
       "      <td>0.950071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>DRESSLER</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.999211</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.893382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ZSN</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.992694</td>\n",
       "      <td>0.652941</td>\n",
       "      <td>0.788845</td>\n",
       "      <td>0.644448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>REC_IM</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.999162</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.863924</td>\n",
       "      <td>0.837950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>P_IM_STEN</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.996689</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.900641</td>\n",
       "      <td>0.842464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>FIBR_PREDS</td>\n",
       "      <td>Naive Bayes - Gaussian</td>\n",
       "      <td>0.717701</td>\n",
       "      <td>0.605882</td>\n",
       "      <td>0.615635</td>\n",
       "      <td>0.686267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>PREDS_TAH</td>\n",
       "      <td>Naive Bayes - Gaussian</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.429412</td>\n",
       "      <td>0.429851</td>\n",
       "      <td>0.589021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>JELUD_TAH</td>\n",
       "      <td>Naive Bayes - Gaussian</td>\n",
       "      <td>0.820751</td>\n",
       "      <td>0.770588</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.852603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>FIBR_JELUD</td>\n",
       "      <td>Naive Bayes - Gaussian</td>\n",
       "      <td>0.785322</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.723602</td>\n",
       "      <td>0.795154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>A_V_BLOK</td>\n",
       "      <td>Naive Bayes - Gaussian</td>\n",
       "      <td>0.727557</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>0.537994</td>\n",
       "      <td>0.676008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>OTEK_LANC</td>\n",
       "      <td>Naive Bayes - Gaussian</td>\n",
       "      <td>0.665417</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.496774</td>\n",
       "      <td>0.611610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>RAZRIV</td>\n",
       "      <td>Naive Bayes - Gaussian</td>\n",
       "      <td>0.821373</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.764350</td>\n",
       "      <td>0.834702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>DRESSLER</td>\n",
       "      <td>Naive Bayes - Gaussian</td>\n",
       "      <td>0.720032</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.549383</td>\n",
       "      <td>0.671451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ZSN</td>\n",
       "      <td>Naive Bayes - Gaussian</td>\n",
       "      <td>0.617146</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.366534</td>\n",
       "      <td>0.443430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>REC_IM</td>\n",
       "      <td>Naive Bayes - Gaussian</td>\n",
       "      <td>0.657586</td>\n",
       "      <td>0.420588</td>\n",
       "      <td>0.398734</td>\n",
       "      <td>0.532019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>P_IM_STEN</td>\n",
       "      <td>Naive Bayes - Gaussian</td>\n",
       "      <td>0.701987</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.464744</td>\n",
       "      <td>0.587617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Variable Objetivo                  Modelo  Accuracy Train  Accuracy Test  \\\n",
       "0         FIBR_PREDS            RandomForest        1.000000       0.897059   \n",
       "1          PREDS_TAH            RandomForest        0.998474       0.985294   \n",
       "2          JELUD_TAH            RandomForest        0.998451       0.979412   \n",
       "3         FIBR_JELUD            RandomForest        1.000000       0.944118   \n",
       "4           A_V_BLOK            RandomForest        1.000000       0.970588   \n",
       "5          OTEK_LANC            RandomForest        1.000000       0.905882   \n",
       "6             RAZRIV            RandomForest        0.998830       0.967647   \n",
       "7           DRESSLER            RandomForest        1.000000       0.950000   \n",
       "8                ZSN            RandomForest        1.000000       0.785294   \n",
       "9             REC_IM            RandomForest        1.000000       0.920588   \n",
       "10         P_IM_STEN            RandomForest        1.000000       0.908824   \n",
       "11        FIBR_PREDS                 XGBoost        0.999581       0.879412   \n",
       "12         PREDS_TAH                 XGBoost        0.997330       0.985294   \n",
       "13         JELUD_TAH                 XGBoost        0.998451       0.967647   \n",
       "14        FIBR_JELUD                 XGBoost        0.981554       0.941176   \n",
       "15          A_V_BLOK                 XGBoost        0.998829       0.970588   \n",
       "16         OTEK_LANC                 XGBoost        0.991667       0.888235   \n",
       "17            RAZRIV                 XGBoost        0.996490       0.964706   \n",
       "18          DRESSLER                 XGBoost        0.991719       0.952941   \n",
       "19               ZSN                 XGBoost        0.883585       0.782353   \n",
       "20            REC_IM                 XGBoost        0.992456       0.911765   \n",
       "21         P_IM_STEN                 XGBoost        0.987169       0.905882   \n",
       "22        FIBR_PREDS                     SVM        1.000000       0.811765   \n",
       "23         PREDS_TAH                     SVM        1.000000       0.973529   \n",
       "24         JELUD_TAH                     SVM        1.000000       0.955882   \n",
       "25        FIBR_JELUD                     SVM        1.000000       0.920588   \n",
       "26          A_V_BLOK                     SVM        1.000000       0.929412   \n",
       "27         OTEK_LANC                     SVM        0.999167       0.864706   \n",
       "28            RAZRIV                     SVM        1.000000       0.952941   \n",
       "29          DRESSLER                     SVM        0.999211       0.882353   \n",
       "30               ZSN                     SVM        0.992694       0.652941   \n",
       "31            REC_IM                     SVM        0.999162       0.811765   \n",
       "32         P_IM_STEN                     SVM        0.996689       0.835294   \n",
       "33        FIBR_PREDS  Naive Bayes - Gaussian        0.717701       0.605882   \n",
       "34         PREDS_TAH  Naive Bayes - Gaussian        0.695652       0.429412   \n",
       "35         JELUD_TAH  Naive Bayes - Gaussian        0.820751       0.770588   \n",
       "36        FIBR_JELUD  Naive Bayes - Gaussian        0.785322       0.717647   \n",
       "37          A_V_BLOK  Naive Bayes - Gaussian        0.727557       0.544118   \n",
       "38         OTEK_LANC  Naive Bayes - Gaussian        0.665417       0.514706   \n",
       "39            RAZRIV  Naive Bayes - Gaussian        0.821373       0.750000   \n",
       "40          DRESSLER  Naive Bayes - Gaussian        0.720032       0.550000   \n",
       "41               ZSN  Naive Bayes - Gaussian        0.617146       0.417647   \n",
       "42            REC_IM  Naive Bayes - Gaussian        0.657586       0.420588   \n",
       "43         P_IM_STEN  Naive Bayes - Gaussian        0.701987       0.485294   \n",
       "\n",
       "    Especificidad  F1_score  \n",
       "0        0.983713  0.867688  \n",
       "1        1.000000  0.977996  \n",
       "2        1.000000  0.969225  \n",
       "3        0.996894  0.919836  \n",
       "4        1.000000  0.958553  \n",
       "5        0.983871  0.880250  \n",
       "6        0.993958  0.957522  \n",
       "7        0.996914  0.928507  \n",
       "8        0.968127  0.745669  \n",
       "9        0.990506  0.890983  \n",
       "10       0.987179  0.878671  \n",
       "11       0.967427  0.853268  \n",
       "12       1.000000  0.977996  \n",
       "13       0.987988  0.963308  \n",
       "14       0.990683  0.923086  \n",
       "15       1.000000  0.958553  \n",
       "16       0.974194  0.857797  \n",
       "17       0.990937  0.956041  \n",
       "18       1.000000  0.929979  \n",
       "19       0.984064  0.730870  \n",
       "20       0.977848  0.890795  \n",
       "21       0.983974  0.877035  \n",
       "22       0.882736  0.820697  \n",
       "23       0.985075  0.974713  \n",
       "24       0.975976  0.957320  \n",
       "25       0.959627  0.919515  \n",
       "26       0.957447  0.934626  \n",
       "27       0.932258  0.860308  \n",
       "28       0.978852  0.950071  \n",
       "29       0.925926  0.893382  \n",
       "30       0.788845  0.644448  \n",
       "31       0.863924  0.837950  \n",
       "32       0.900641  0.842464  \n",
       "33       0.615635  0.686267  \n",
       "34       0.429851  0.589021  \n",
       "35       0.783784  0.852603  \n",
       "36       0.723602  0.795154  \n",
       "37       0.537994  0.676008  \n",
       "38       0.496774  0.611610  \n",
       "39       0.764350  0.834702  \n",
       "40       0.549383  0.671451  \n",
       "41       0.366534  0.443430  \n",
       "42       0.398734  0.532019  \n",
       "43       0.464744  0.587617  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados_modelo_metricas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a78e150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "# Lectura\n",
    "X_train = X_TRAIN_DF.loc[X_TRAIN_DF[\"var_objetivo\"]==\"FIBR_PREDS\"].dropna(axis='columns')\n",
    "y_train = Y_TRAIN_DF.loc[Y_TRAIN_DF[\"var_objetivo\"]==\"FIBR_PREDS\", \"FIBR_PREDS\"]\n",
    "X_test = X_TEST_DF.loc[X_TEST_DF[\"var_objetivo\"]==\"FIBR_PREDS\"].dropna(axis='columns')\n",
    "y_test = Y_TEST_DF.loc[Y_TEST_DF[\"var_objetivo\"]==\"FIBR_PREDS\", \"FIBR_PREDS\"]\n",
    "X_train.drop([\"var_objetivo\"], axis=1, inplace=True)\n",
    "X_test.drop([\"var_objetivo\"], axis=1, inplace=True)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train.iloc[0].shape)),\n",
    "    tf.keras.layers.Dense(600, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(80, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(60, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4835d6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2384, 66)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9572446d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0.0\n",
       "4       0.0\n",
       "       ... \n",
       "2379    1.0\n",
       "2380    1.0\n",
       "2381    1.0\n",
       "2382    1.0\n",
       "2383    1.0\n",
       "Name: FIBR_PREDS, Length: 2384, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "203a9b08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.4366 - accuracy: 0.5306 - val_loss: 1.3153 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6467 - accuracy: 0.6604 - val_loss: 0.7963 - val_accuracy: 0.2346\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5602 - accuracy: 0.7192 - val_loss: 0.6887 - val_accuracy: 0.5112\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5079 - accuracy: 0.7463 - val_loss: 0.6837 - val_accuracy: 0.5447\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4709 - accuracy: 0.7606 - val_loss: 0.4810 - val_accuracy: 0.8045\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4296 - accuracy: 0.8006 - val_loss: 0.5864 - val_accuracy: 0.7179\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4004 - accuracy: 0.8169 - val_loss: 0.5957 - val_accuracy: 0.6983\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3620 - accuracy: 0.8470 - val_loss: 0.5505 - val_accuracy: 0.7374\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3073 - accuracy: 0.8939 - val_loss: 0.4098 - val_accuracy: 0.8603\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2617 - accuracy: 0.9245 - val_loss: 0.3495 - val_accuracy: 0.9134\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2256 - accuracy: 0.9388 - val_loss: 0.4087 - val_accuracy: 0.8715\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1859 - accuracy: 0.9546 - val_loss: 0.2804 - val_accuracy: 0.9358\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1557 - accuracy: 0.9669 - val_loss: 0.1899 - val_accuracy: 0.9441\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1351 - accuracy: 0.9654 - val_loss: 0.1930 - val_accuracy: 0.9497\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1092 - accuracy: 0.9808 - val_loss: 0.1789 - val_accuracy: 0.9525\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0905 - accuracy: 0.9877 - val_loss: 0.0963 - val_accuracy: 0.9749\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9877 - val_loss: 0.1353 - val_accuracy: 0.9665\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0652 - accuracy: 0.9901 - val_loss: 0.1170 - val_accuracy: 0.9693\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0545 - accuracy: 0.9946 - val_loss: 0.0543 - val_accuracy: 0.9972\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0472 - accuracy: 0.9951 - val_loss: 0.0585 - val_accuracy: 0.9944\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9975 - val_loss: 0.0445 - val_accuracy: 0.9972\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0341 - accuracy: 0.9980 - val_loss: 0.0508 - val_accuracy: 0.9972\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0282 - accuracy: 0.9990 - val_loss: 0.0437 - val_accuracy: 0.9972\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0254 - accuracy: 0.9990 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.9995 - val_loss: 0.0334 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0211 - accuracy: 1.0000 - val_loss: 0.0307 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0176 - accuracy: 0.9995 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0296 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0185 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = tf.optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_split=0.15, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3deae382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6749 - accuracy: 0.8588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6749401688575745, 0.8588235378265381]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf11780",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
